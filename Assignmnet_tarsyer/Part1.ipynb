{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Feb 21 22:41:12 2020\n",
    "\n",
    "@author: GUR45397\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "import os\n",
    "\n",
    "trainingDataPath = \"datasets/mnist/trainingSample/\"\n",
    "testingDataPath = \"datasets/mnist/testset/\" \n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "var = 0.1\n",
    "sigma = var**0.5\n",
    "gaussisanNoise = np.random.normal(mean,sigma,(height,width,channels))\n",
    "gaussisanNoise = gaussisanNoise.reshape(height,width,channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, forTraining):\n",
    "    if(True == forTraining):\n",
    "        path = trainingDataPath\n",
    "    else:\n",
    "        path = testingDataPath\n",
    "    imagesName = os.listdir(path)\n",
    "    for batch_i in range(0, len(imagesName)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        try:\n",
    "            batch = imagesName[start_i:start_i + batch_size]           \n",
    "        except IndexError:\n",
    "            batch = imagesName[start_i:]\n",
    "        noisedImageList = []\n",
    "        actualImageList = []\n",
    "        for image in batch:\n",
    "            imageData = io.imread(path+'/'+image)\n",
    "            imageData = imageData/255\n",
    "            imageData = imageData.reshape(width,height,channels)\n",
    "            \n",
    "            actualImageList.append(imageData)\n",
    "            noisedImageList.append(imageData+gaussisanNoise)\n",
    "        yield(np.array(noisedImageList), np.array(actualImageList))\n",
    "        \n",
    "#for i, (X_image,angle) in enumerate(get_batch(100)):\n",
    "#    print(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"convLayer1/Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convLayer2/Relu:0\", shape=(?, 14, 14, 32), dtype=float32) Tensor(\"max_pooling2d_1/MaxPool:0\", shape=(?, 7, 7, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "inputVal = tf.compat.v1.placeholder(tf.float32, shape = [None ,height,width,channels],name = 'features')\n",
    "output = tf.compat.v1.placeholder(tf.float32, shape = [None,height,width,channels],name = 'output')\n",
    "lr = tf.compat.v1.placeholder(tf.float32, name = 'lr')\n",
    "\n",
    "\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs=inputVal, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu, name=\"convLayer1\")\n",
    "\n",
    "print(conv1)\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "conv2 = tf.layers.conv2d(inputs=maxpool1, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu, name=\"convLayer2\")\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "print(conv2, maxpool2)\n",
    "\n",
    "conv3 = tf.layers.conv2d(inputs=maxpool2, filters=16, kernel_size=(3,3), padding='same', activation=None, name=\"convLayer3\")\n",
    "encoded = tf.layers.max_pooling2d(tf.nn.relu(conv3), pool_size=(2,2), strides=(2,2), padding='same')\n",
    "midlayer = tf.identity(conv3,\"bottle_neck_layer\")\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_images(encoded, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv4 = tf.layers.conv2d(inputs=upsample1, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "upsample2 = tf.image.resize_images(conv4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv5 = tf.layers.conv2d(inputs=upsample2, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "upsample3 = tf.image.resize_images(conv5, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv6 = tf.layers.conv2d(inputs=upsample3, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n",
    "decoded = tf.layers.conv2d(inputs=conv6, filters=1, kernel_size=(3,3), padding='same', activation=None)\n",
    "\n",
    "residual_error = tf.pow(decoded-inputVal,2)\n",
    "cost = tf.reduce_mean(residual_error) \n",
    "\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(cost, name=\"optimizer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Train: 0.18752186 test: 0.17590873 i: 0\n",
      "******************Train: 0.1711989 test: 0.16605169 i: 1\n",
      "******************Train: 0.16368745 test: 0.1606621 i: 2\n",
      "******************Train: 0.15900376 test: 0.15598531 i: 3\n",
      "******************Train: 0.15495028 test: 0.1519406 i: 4\n",
      "******************Train: 0.15093909 test: 0.14795493 i: 5\n",
      "******************Train: 0.14687854 test: 0.14372356 i: 6\n",
      "******************Train: 0.1428446 test: 0.14040484 i: 7\n",
      "******************Train: 0.13938662 test: 0.13657524 i: 8\n",
      "******************Train: 0.13547833 test: 0.1321939 i: 9\n",
      "[ 3.6320342e-03 -4.4149416e-03 -1.1061602e-05  5.5956347e-03\n",
      "  6.1686570e-03 -2.6084653e-03  3.1318343e-03  6.2185284e-03\n",
      " -8.3545683e-04  3.7256642e-03 -2.7629605e-04 -5.5981535e-03\n",
      "  1.4219645e-03  7.4025029e-03  4.3555531e-03 -1.8524997e-03\n",
      " -4.9785725e-03 -5.5164807e-03  1.9329223e-03  1.1189973e-02\n",
      "  1.4510464e-02  1.0013958e-02 -4.2620869e-03  3.6067588e-03\n",
      " -4.4161984e-04 -8.5923700e-03  2.1126161e-03  5.8145835e-03\n",
      " -1.2200944e-03  4.3278490e-03  6.1605722e-03  7.2474248e-04]\n"
     ]
    }
   ],
   "source": [
    "epoch=10\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    summ_writer = tf.compat.v1.summary.FileWriter(\"out\",sess.graph)\n",
    "    init.run()\n",
    "    stime = time.time()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        average_loss = []\n",
    "        for batch_index, (noisedImage, actualImage) in enumerate(get_batch(\n",
    "                                            batch_size=100, forTraining=True)):\n",
    "            _,loss = sess.run((train_op, cost), \n",
    "                                 feed_dict={inputVal:noisedImage, \n",
    "                                                output:actualImage,\n",
    "                                                lr:0.001})\n",
    "            average_loss.append(loss)\n",
    "            \n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=\"Trainloss\", simple_value=np.mean(average_loss))\n",
    "\n",
    "        summ_writer.add_summary(summary, i)\n",
    "        summ_writer.flush()\n",
    "\n",
    "        for batch_index, (noisedImage, actualImage) in enumerate(get_batch(\n",
    "                                        batch_size=100, forTraining=False)):\n",
    "            m, loss = sess.run((midlayer, cost), \n",
    "                                 feed_dict={inputVal:noisedImage})\n",
    "               \n",
    "\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag=\"Validloss\", simple_value=loss)\n",
    "\n",
    "            summ_writer.add_summary(summary, i)\n",
    "            summ_writer.flush()\n",
    "            print(\"******************Train:\",np.mean(average_loss), \"test:\",loss,\"i:\", i) \n",
    "            break\n",
    "    #print(tf.trainable_variables())        \n",
    "    c1_w = tf.trainable_variables()[0].eval()\n",
    "    c2_w = tf.trainable_variables()[2].eval()\n",
    "    c3_w = tf.trainable_variables()[4].eval()\n",
    "    \n",
    "    print(tf.trainable_variables()[1].eval())\n",
    "    saver.save(sess,\"out/graph/tensorflowModel.ckpt\")\n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), \"out/graph\", 'tensorflowModel.pbtxt', as_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from out/graph/tensorflowModel.ckpt\n",
      "INFO:tensorflow:Froze 44 variables.\n",
      "INFO:tensorflow:Converted 44 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph.freeze_graph('out/graph/tensorflowModel.pbtxt', \"\", \n",
    "                              False, \n",
    "                              'out/graph/tensorflowModel.ckpt', \n",
    "                              \"features, bottle_neck_layer, optimizer\", #Since using optimizer, as well it will have\n",
    "                              #the full graph details thats why size would be as same as \"out/graph/tensorflowModel.ckpt'\"\n",
    "                              \"save/restore_all\", \"save/Const:0\", #not relevant\n",
    "                              'out/graph/frozentensorflowModel.pb', \n",
    "                              True, \"\"  \n",
    "                         )\n",
    "    \n",
    "inputGraph = tf.GraphDef()\n",
    "with tf.gfile.Open(\"out/graph/frozentensorflowModel.pb\",\"rb\") as f:\n",
    "    data2read = f.read()\n",
    "    inputGraph.ParseFromString(data2read)\n",
    "outputGraph = optimize_for_inference_lib.optimize_for_inference(\n",
    "                inputGraph,\n",
    "                [\"features\"],\n",
    "                [\"bottle_neck_layer\"],#optimizer not considered, size is small.\n",
    "                tf.float32.as_datatype_enum)\n",
    "f = tf.gfile.FastGFile('out/graph/OptimizedGraph.pb', \"w\")\n",
    "f.write(outputGraph.SerializeToString()) \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 1231 (pid 1124), started 0:00:27 ago. (Use '!kill 1124' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-69f8ee3a369a6685\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-69f8ee3a369a6685\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 1231;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"out\" --port 1231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def load_graph():\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with open(\"out/graph/OptimizedGraph.pb\",\"rb\") as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with graph.as_default():\n",
    "        tf.graph_util.import_graph_def(graph_def)\n",
    "        \n",
    "        #tf.import_graph_def(graph_def)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "graph = load_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1, 7, 7, 16)\n"
     ]
    }
   ],
   "source": [
    "for i, (X_image,angle) in enumerate(get_batch(1, False)):\n",
    "    print()\n",
    "    break\n",
    "input_name = \"import/features\"\n",
    "\n",
    "\n",
    "output_Encoder_features_op = \"import/bottle_neck_layer\"\n",
    "\n",
    "input_operation = graph.get_operation_by_name(input_name);\n",
    "\n",
    "output_Encoder_features = graph.get_operation_by_name(output_Encoder_features_op);\n",
    "\n",
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    results = sess.run(output_Encoder_features.outputs[0],\n",
    "                      {input_operation.outputs[0]: X_image})\n",
    "    print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trainingDataPathforclass = \"datasets/mnist/mnist_train.csv\"\n",
    "testingDataPathforclass = \"datasets/mnist/mnist_test.csv\" \n",
    "\n",
    "def get_batch_for_classification(batch_size, forTraining):\n",
    "    if(True == forTraining):\n",
    "        path = trainingDataPathforclass\n",
    "    else:\n",
    "        path = testingDataPathforclass\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df = df[0:1000]\n",
    "    labels = df['label']\n",
    "    imageData = df.iloc[:,1:]\n",
    "    imageBatch = []\n",
    "    for batch_i in range(0, len(labels)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        try:\n",
    "            batch = imageData[start_i:start_i + batch_size]  \n",
    "            batch_l = labels[start_i:start_i + batch_size]\n",
    "        except IndexError:\n",
    "            batch = imageData[start_i:]\n",
    "            batch_l = labels[start_i:]\n",
    "         \n",
    "        batch = batch/255\n",
    "        batch = np.array(batch)\n",
    "        batch =  batch.reshape(batch_size,28,28,1)+gaussisanNoise\n",
    "        #batch = batch.reshape(batch_size,28,28,1)\n",
    "        yield(batch, np.array(batch_l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = 10\n",
    "h = 28\n",
    "w = 28\n",
    "c = 1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "encoded_input =  tf.compat.v1.placeholder(tf.float32, shape = [None, h, w, c],name = 'encoded_input')\n",
    "label = tf.compat.v1.placeholder(tf.int32, shape = [None],name = 'label')\n",
    "rate = tf.compat.v1.placeholder(tf.float32, name = 'rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filter1 = tf.compat.v1.get_variable('weights11', initializer= c1_w, dtype=tf.float32, trainable=False)    \n",
    "conv1 = tf.nn.conv2d(encoded_input, filter1, strides = 1, padding='SAME', name = 'conv1')\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "\n",
    "filter2 = tf.compat.v1.get_variable('weights12',initializer= c2_w,dtype=tf.float32,trainable=False)    \n",
    "conv2 = tf.nn.conv2d(maxpool1, filter2, strides = 1, padding='SAME', name = 'conv2')\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "\n",
    "filter3 = tf.compat.v1.get_variable('weights13',initializer= c3_w,dtype=tf.float32, trainable=False)    \n",
    "conv3 = tf.nn.conv2d(maxpool2, filter3, strides = 1, padding='SAME', name = 'conv3')\n",
    "conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "conv3_reshaped = tf.reshape(conv3,shape = (-1,conv3.shape[1]*conv3.shape[2]*conv3.shape[3]))\n",
    "\n",
    "\n",
    "dense1  = tf.compat.v1.layers.dense(conv3_reshaped,512,activation=tf.nn.relu)\n",
    "dense2  = tf.compat.v1.layers.dense(dense1,128,activation=tf.nn.relu)\n",
    "logits  = tf.compat.v1.layers.dense(dense2,output,name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = tf.compat.v1.train.AdamOptimizer(learning_rate=rate)\n",
    "op = gd.minimize(loss, name=\"GD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Train: 2.0400944 test: 2.4007676 i: 0\n",
      "******************Train: 1.4604642 test: 2.7808564 i: 1\n",
      "******************Train: 1.0766575 test: 3.4482934 i: 2\n",
      "******************Train: 0.84183854 test: 4.1167474 i: 3\n",
      "******************Train: 0.71602714 test: 4.629122 i: 4\n",
      "******************Train: 0.65721637 test: 4.9901714 i: 5\n",
      "******************Train: 0.62598217 test: 5.2901845 i: 6\n",
      "******************Train: 0.59936595 test: 5.5108213 i: 7\n",
      "******************Train: 0.5155404 test: 5.765572 i: 8\n",
      "******************Train: 0.45746034 test: 5.967716 i: 9\n"
     ]
    }
   ],
   "source": [
    "epoch=10\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    summ_writer = tf.compat.v1.summary.FileWriter(\"out2\",sess.graph)\n",
    "    init.run()\n",
    "    stime = time.time()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        average_loss = []\n",
    "        for batch_index, (noisedImage, labels) in enumerate(get_batch_for_classification(\n",
    "                                            batch_size=100, forTraining=True)):\n",
    "            _,l = sess.run((op, loss), \n",
    "                                 feed_dict={encoded_input:noisedImage, \n",
    "                                                label:labels,\n",
    "                                                rate:0.001})\n",
    "            #print(l)\n",
    "            average_loss.append(l)\n",
    "            \n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=\"CTrainloss\", simple_value=np.mean(average_loss))\n",
    "\n",
    "        summ_writer.add_summary(summary, i)\n",
    "        summ_writer.flush()\n",
    "\n",
    "        for batch_index, (noisedImage, actualImage) in enumerate(get_batch_for_classification(\n",
    "                                            batch_size=100, forTraining=False)):\n",
    "            l = sess.run(loss, feed_dict={encoded_input:noisedImage,label:labels})\n",
    "               \n",
    "\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag=\"CValidloss\", simple_value=l)\n",
    "\n",
    "            summ_writer.add_summary(summary, i)\n",
    "            summ_writer.flush()\n",
    "            print(\"******************Train:\",np.mean(average_loss), \"test:\",l,\"i:\", i) \n",
    "            break\n",
    "    saver.save(sess,\"out2/graph/tensorflowModel2.ckpt\")\n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), \"out2/graph\", 'tensorflowModel2.pbtxt', as_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 9242 (pid 8716), started 0:00:05 ago. (Use '!kill 8716' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c2e308886b85aaa1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c2e308886b85aaa1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 9242;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"out2\" --port 9242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
